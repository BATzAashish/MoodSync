{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":256618,"sourceType":"datasetVersion","datasetId":107620},{"sourceId":1351797,"sourceType":"datasetVersion","datasetId":786787},{"sourceId":4558658,"sourceType":"datasetVersion","datasetId":2660706},{"sourceId":344596,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":288079,"modelId":308857},{"sourceId":344598,"sourceType":"modelInstanceVersion","isSourceIdPinned":false,"modelInstanceId":288081,"modelId":308859}],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import joblib\nfrom keras.models import load_model\n\n# Load audio emotion model (already trained and saved)\naudio_model = joblib.load(\"/kaggle/input/audio-emotion/other/default/1/emotion_model.pkl\")\n\n# Load facial emotion model (already trained and saved)\nface_model = load_model(\"/kaggle/input/face-emotion/other/default/1/model_optimal.h5\")\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:05.560889Z","iopub.execute_input":"2025-05-01T04:03:05.561679Z","iopub.status.idle":"2025-05-01T04:03:30.256632Z","shell.execute_reply.started":"2025-05-01T04:03:05.561644Z","shell.execute_reply":"2025-05-01T04:03:30.255832Z"}},"outputs":[{"name":"stderr","text":"2025-05-01 04:03:08.651378: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1746072189.064818      31 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1746072189.184615      31 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator StandardScaler from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator LogisticRegression from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/sklearn/base.py:318: UserWarning: Trying to unpickle estimator Pipeline from version 1.0.2 when using version 1.2.2. This might lead to breaking code or invalid results. Use at your own risk. For more info please refer to:\nhttps://scikit-learn.org/stable/model_persistence.html#security-maintainability-limitations\n  warnings.warn(\n/usr/local/lib/python3.11/dist-packages/keras/src/layers/convolutional/base_conv.py:107: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\nI0000 00:00:1746072207.801551      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 13942 MB memory:  -> device: 0, name: Tesla T4, pci bus id: 0000:00:04.0, compute capability: 7.5\nI0000 00:00:1746072207.802252      31 gpu_device.cc:2022] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 13942 MB memory:  -> device: 1, name: Tesla T4, pci bus id: 0000:00:05.0, compute capability: 7.5\n/usr/local/lib/python3.11/dist-packages/keras/src/optimizers/base_optimizer.py:33: UserWarning: Argument `decay` is no longer supported and will be ignored.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"import librosa\nimport numpy as np\n\ndef predict_audio_emotion(audio_path):\n    x, sr = librosa.load(audio_path)\n    mfcc = np.mean(librosa.feature.mfcc(y=x, sr=sr, n_mfcc=128), axis=1).reshape(1, -1)\n    pred = audio_model.predict(mfcc)[0]\n    return \"happy\" if pred == 0 else \"sad\"\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:30.258049Z","iopub.execute_input":"2025-05-01T04:03:30.258551Z","iopub.status.idle":"2025-05-01T04:03:30.269493Z","shell.execute_reply.started":"2025-05-01T04:03:30.258531Z","shell.execute_reply":"2025-05-01T04:03:30.268744Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from keras.preprocessing import image\n\nlabel_dict = {0:'Angry',1:'Disgust',2:'Fear',3:'Happy',4:'Neutral',5:'Sad',6:'Surprise'}\n\ndef predict_face_emotion(image_path):\n    img = image.load_img(image_path, target_size=(48, 48), color_mode=\"grayscale\")\n    img = np.array(img).reshape(1, 48, 48, 1) / 255.0\n    result = face_model.predict(img)\n    label_idx = np.argmax(result[0])\n    return label_dict[label_idx]\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:30.270125Z","iopub.execute_input":"2025-05-01T04:03:30.270322Z","iopub.status.idle":"2025-05-01T04:03:30.285488Z","shell.execute_reply.started":"2025-05-01T04:03:30.270300Z","shell.execute_reply":"2025-05-01T04:03:30.284922Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"def combine_emotions(audio_path, face_image_path):\n    audio_emotion = predict_audio_emotion(audio_path)\n    face_emotion = predict_face_emotion(face_image_path)\n    \n    print(f\"Audio Emotion: {audio_emotion}\")\n    print(f\"Facial Emotion: {face_emotion}\")\n    \n    # Simple fusion logic (example: if both agree, use that; else prioritize face)\n    if audio_emotion.lower() in face_emotion.lower():\n        final = face_emotion\n    elif face_emotion in ['Happy', 'Sad']:\n        final = face_emotion\n    else:\n        final = audio_emotion.capitalize()\n    \n    print(f\"Final Multimodal Emotion: {final}\")\n    return final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:30.287271Z","iopub.execute_input":"2025-05-01T04:03:30.287445Z","iopub.status.idle":"2025-05-01T04:03:30.301188Z","shell.execute_reply.started":"2025-05-01T04:03:30.287432Z","shell.execute_reply":"2025-05-01T04:03:30.300663Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"audio_path = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\"\nface_image_path = \"/kaggle/input/fer2013/train/happy/Training_10019449.jpg\"\n\ncombine_emotions(audio_path, face_image_path)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:30.301657Z","iopub.execute_input":"2025-05-01T04:03:30.301810Z","iopub.status.idle":"2025-05-01T04:03:47.845388Z","shell.execute_reply.started":"2025-05-01T04:03:30.301798Z","shell.execute_reply":"2025-05-01T04:03:47.844523Z"}},"outputs":[{"name":"stderr","text":"WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nI0000 00:00:1746072225.450901      92 service.cc:148] XLA service 0x787d30003910 initialized for platform CUDA (this does not guarantee that XLA will be used). Devices:\nI0000 00:00:1746072225.452533      92 service.cc:156]   StreamExecutor device (0): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746072225.452572      92 service.cc:156]   StreamExecutor device (1): Tesla T4, Compute Capability 7.5\nI0000 00:00:1746072225.665214      92 cuda_dnn.cc:529] Loaded cuDNN version 90300\n","output_type":"stream"},{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 3s/step\nAudio Emotion: sad\nFacial Emotion: Happy\nFinal Multimodal Emotion: Happy\n","output_type":"stream"},{"name":"stderr","text":"I0000 00:00:1746072227.830743      92 device_compiler.h:188] Compiled cluster using XLA!  This line is logged at most once for the lifetime of the process.\n","output_type":"stream"},{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"'Happy'"},"metadata":{}}],"execution_count":5},{"cell_type":"code","source":"import pandas as pd\n\ndf = pd.read_csv(\"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\")\nprint(df.columns)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:47.846255Z","iopub.execute_input":"2025-05-01T04:03:47.847324Z","iopub.status.idle":"2025-05-01T04:03:49.464210Z","shell.execute_reply.started":"2025-05-01T04:03:47.847297Z","shell.execute_reply":"2025-05-01T04:03:49.463544Z"}},"outputs":[{"name":"stdout","text":"Index(['artist', 'song', 'link', 'text'], dtype='object')\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"df = df.dropna(subset=[\"text\", \"artist\", \"song\"])\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:03:49.464839Z","iopub.execute_input":"2025-05-01T04:03:49.465058Z","iopub.status.idle":"2025-05-01T04:03:49.497041Z","shell.execute_reply.started":"2025-05-01T04:03:49.465042Z","shell.execute_reply":"2025-05-01T04:03:49.496231Z"}},"outputs":[],"execution_count":7},{"cell_type":"code","source":"# Clean lyrics\nimport re\ndef clean_lyrics(text):\n    text = re.sub(r'\\[.*?\\]', '', text)\n    text = re.sub(r'\\n', ' ', text)\n    text = re.sub(r'[^a-zA-Z ]', '', text)\n    text = text.lower()\n    return text\n\ndf[\"clean_lyrics\"] = df[\"text\"].apply(clean_lyrics)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:20:11.212435Z","iopub.execute_input":"2025-05-01T04:20:11.213147Z","iopub.status.idle":"2025-05-01T04:20:13.141782Z","shell.execute_reply.started":"2025-05-01T04:20:11.213121Z","shell.execute_reply":"2025-05-01T04:20:13.141025Z"}},"outputs":[],"execution_count":9},{"cell_type":"code","source":"import pandas as pd\nimport numpy as np\nimport re\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom sklearn.metrics.pairwise import cosine_similarity\nimport nltk\n\n# Download NLTK stopwords\nnltk.download('stopwords')\nfrom nltk.corpus import stopwords\n\n# Load the dataset\ndf = pd.read_csv(\"/kaggle/input/spotify-million-song-dataset/spotify_millsongdata.csv\")\n\n# Show column names to verify structure (optional)\nprint(\"Columns:\", df.columns)\n\n# Drop rows with missing values\ndf = df.dropna(subset=[\"text\", \"artist\", \"song\"])\n\n# Clean lyrics/text\ndef clean_lyrics(text):\n    text = re.sub(r'\\[.*?\\]', '', text)        # Remove [Verse], [Chorus], etc.\n    text = re.sub(r'\\n', ' ', text)            # Replace newlines with space\n    text = re.sub(r'[^a-zA-Z ]', '', text)     # Remove punctuation/special chars\n    text = text.lower()\n    return text\n\ndf[\"clean_lyrics\"] = df[\"text\"].apply(clean_lyrics)\n\n# Sample for performance\ndf = df.sample(5000, random_state=42).reset_index(drop=True)\n\n# Create TF-IDF Matrix\ntfidf = TfidfVectorizer(stop_words=stopwords.words('english'), max_features=5000)\ntfidf_matrix = tfidf.fit_transform(df[\"clean_lyrics\"])\n\n# Compute cosine similarity matrix\ncos_sim = cosine_similarity(tfidf_matrix)\n\n# Recommend similar songs based on lyrics\ndef recommend_songs(song_title, top_n=20):\n    idx = df[df['song'].str.lower() == song_title.lower()].index\n    if len(idx) == 0:\n        print(\" Song not found in dataset!\")\n        return\n    idx = idx[0]\n    sim_scores = list(enumerate(cos_sim[idx]))\n    sim_scores = sorted(sim_scores, key=lambda x: x[1], reverse=True)\n    sim_scores = sim_scores[1:top_n+1]\n\n    print(f\"\\nüéµ Recommendations based on '{df.iloc[idx]['song']}' by {df.iloc[idx]['artist']}:\")\n    for i, (song_idx, score) in enumerate(sim_scores):\n        song = df.iloc[song_idx]\n        print(f\"{i+1}. {song['song']} by {song['artist']} (Similarity Score: {score:.2f})\")\n\n# Example usage\nrecommend_songs(\"Everything Must Change\")  # Replace with a song title from the dataset\n\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:20:48.402352Z","iopub.execute_input":"2025-05-01T04:20:48.402629Z","iopub.status.idle":"2025-05-01T04:20:54.218190Z","shell.execute_reply.started":"2025-05-01T04:20:48.402606Z","shell.execute_reply":"2025-05-01T04:20:54.217432Z"}},"outputs":[{"name":"stderr","text":"[nltk_data] Downloading package stopwords to /usr/share/nltk_data...\n[nltk_data]   Package stopwords is already up-to-date!\n","output_type":"stream"},{"name":"stdout","text":"Columns: Index(['artist', 'song', 'link', 'text'], dtype='object')\n\nüéµ Recommendations based on 'Everything Must Change' by Nina Simone:\n1. Fly Into The Sun by Lou Reed (Similarity Score: 0.28)\n2. Here Comes The Sun by Demi Lovato (Similarity Score: 0.27)\n3. Silvery Rain by Olivia Newton-John (Similarity Score: 0.27)\n4. It's Just The Sun by Don McLean (Similarity Score: 0.27)\n5. No More Rain by Kylie Minogue (Similarity Score: 0.25)\n6. The Surest Things Can Change by Gino Vannelli (Similarity Score: 0.25)\n7. Sun Showers by Louis Armstrong (Similarity Score: 0.24)\n8. Rain by Ian Hunter (Similarity Score: 0.23)\n9. One Of The Mysteries Of Life by Tom T. Hall (Similarity Score: 0.23)\n10. Lazy Old Sun by Kinks (Similarity Score: 0.21)\n11. Blind Man by Aerosmith (Similarity Score: 0.21)\n12. Night Lights by Nat King Cole (Similarity Score: 0.21)\n13. Rain by Kiss (Similarity Score: 0.21)\n14. Flashing Lights by Kanye West (Similarity Score: 0.20)\n15. Red Lights by Uriah Heep (Similarity Score: 0.20)\n16. Danger Bird by Neil Young (Similarity Score: 0.20)\n17. Angel by Rod Stewart (Similarity Score: 0.19)\n18. This Is The One by Stone Roses (Similarity Score: 0.19)\n19. One Life by Queensryche (Similarity Score: 0.18)\n20. Memories by Backstreet Boys (Similarity Score: 0.18)\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"def recommend_by_mood(detected_mood, top_n=20):\n    keywords = mood_keywords.get(detected_mood.lower(), [])\n    if not keywords:\n        print(\"üòï No matching keywords for mood:\", detected_mood)\n        return\n\n    mask = df[\"clean_lyrics\"].apply(lambda x: any(word in x for word in keywords))\n    filtered_df = df[mask]\n    if filtered_df.empty:\n        print(\"üôÅ No matching songs found for this mood.\")\n        return\n\n    sampled = filtered_df.sample(min(top_n, len(filtered_df)))\n    print(f\"\\nüéß Mood: {detected_mood.upper()} ‚Äî Top {len(sampled)} Song Recommendations:\\n\")\n    for i, row in sampled.iterrows():\n        print(f\"{row['song']} by {row['artist']}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:21:18.518414Z","iopub.execute_input":"2025-05-01T04:21:18.518699Z","iopub.status.idle":"2025-05-01T04:21:18.524274Z","shell.execute_reply.started":"2025-05-01T04:21:18.518680Z","shell.execute_reply":"2025-05-01T04:21:18.523541Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"mood_keywords = {\n    \"happy\": [\"happy\", \"joy\", \"smile\", \"sun\", \"celebrate\", \"dance\", \"love\"],\n    \"sad\": [\"cry\", \"tears\", \"alone\", \"heartbreak\", \"miss\", \"lost\", \"pain\"],\n    \"angry\": [\"fire\", \"rage\", \"fight\", \"scream\", \"hate\", \"revenge\"],\n    \"calm\": [\"peace\", \"calm\", \"breeze\", \"slow\", \"relax\", \"soft\"],\n    \"excited\": [\"party\", \"tonight\", \"wild\", \"jump\", \"high\", \"energy\"],\n    \"neutral\": [\"life\", \"time\", \"way\", \"dream\", \"go\", \"day\"]\n}\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:21:19.766282Z","iopub.execute_input":"2025-05-01T04:21:19.766746Z","iopub.status.idle":"2025-05-01T04:21:19.771084Z","shell.execute_reply.started":"2025-05-01T04:21:19.766725Z","shell.execute_reply":"2025-05-01T04:21:19.770294Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"def combine_emotions(audio_path, face_image_path):\n    audio_emotion = predict_audio_emotion(audio_path)\n    face_emotion = predict_face_emotion(face_image_path)\n    \n    print(f\"Audio Emotion: {audio_emotion}\")\n    print(f\"Facial Emotion: {face_emotion}\")\n    \n    if audio_emotion.lower() in face_emotion.lower():\n        final = face_emotion\n    elif face_emotion in ['Happy', 'Sad']:\n        final = face_emotion\n    else:\n        final = audio_emotion.capitalize()\n\n    final = final.lower()  # Normalize for mood matching\n    print(f\"Final Multimodal Emotion: {final}\")\n    return final\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:21:20.388273Z","iopub.execute_input":"2025-05-01T04:21:20.388451Z","iopub.status.idle":"2025-05-01T04:21:20.392875Z","shell.execute_reply.started":"2025-05-01T04:21:20.388438Z","shell.execute_reply":"2025-05-01T04:21:20.392166Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Paths to test audio and image\naudio_path = \"/kaggle/input/ravdess-emotional-speech-audio/Actor_01/03-01-01-01-01-01-01.wav\"\nface_image_path = \"/kaggle/input/fer2013/train/happy/Training_10019449.jpg\"\n\n# Detect emotion\nfinal_mood = combine_emotions(audio_path, face_image_path)\n\n# Recommend songs based on detected mood\nrecommend_by_mood(final_mood)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:21:21.979515Z","iopub.execute_input":"2025-05-01T04:21:21.979772Z","iopub.status.idle":"2025-05-01T04:21:22.100504Z","shell.execute_reply.started":"2025-05-01T04:21:21.979754Z","shell.execute_reply":"2025-05-01T04:21:22.099885Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\nAudio Emotion: sad\nFacial Emotion: Happy\nFinal Multimodal Emotion: happy\n\nüéß Mood: HAPPY ‚Äî Top 20 Song Recommendations:\n\nThat's The Way Of The World by Incognito\nSnow by Bing Crosby\nNever On Sunday by Andy Williams\nI Love Rock 'n' Roll by Britney Spears\nDeeper by Hillsong United\nI'll Kiss You by Cyndi Lauper\nI Don't Believe You by Air Supply\nFace To The Highway by Tom Waits\nRoc Me Out by Rihanna\nWhile There's Still Time by Styx\nDrugs Or Jesus by Tim McGraw\nTill They Take My Heart Away by Kyla\nI Found Love With You by Dusty Springfield\nO Come All Ye Faithful by Mariah Carey\nSomething Better by Marianne Faithfull\nCross On The Highway by Hank Williams Jr.\nI'm Waiting For The Day by Beach Boys\nI Hurt You by Pretenders\nGoodbye's by Celine Dion\nOnly Love Would Know by Gordon Lightfoot\n","output_type":"stream"}],"execution_count":20},{"cell_type":"code","source":"import sounddevice as sd\nimport librosa\nimport numpy as np\nimport cv2\nimport threading\nimport time\nfrom keras.models import load_model\nimport joblib\nfrom keras.preprocessing import image\n\n# Load your models (audio + face)\naudio_model = joblib.load(\"/kaggle/input/audio-emotion/other/default/1/emotion_model.pkl\")\nface_model = load_model(\"/kaggle/input/face-emotion/other/default/1/model_optimal.h5\")\n\n# Define the mood keywords (for song recommendations)\nmood_keywords = {\n    \"happy\": [\"happy\", \"joy\", \"smile\", \"sun\", \"celebrate\", \"dance\", \"love\"],\n    \"sad\": [\"cry\", \"tears\", \"alone\", \"heartbreak\", \"miss\", \"lost\", \"pain\"],\n    \"angry\": [\"fire\", \"rage\", \"fight\", \"scream\", \"hate\", \"revenge\"],\n    \"calm\": [\"peace\", \"calm\", \"breeze\", \"slow\", \"relax\", \"soft\"],\n    \"excited\": [\"party\", \"tonight\", \"wild\", \"jump\", \"high\", \"energy\"],\n    \"neutral\": [\"life\", \"time\", \"way\", \"dream\", \"go\", \"day\"]\n}\n\n# Define emotion labels for the face model\nlabel_dict = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n\n# Function to predict audio emotion\ndef predict_audio_emotion(audio_data):\n    mfcc = np.mean(librosa.feature.mfcc(y=audio_data, sr=16000, n_mfcc=128), axis=1).reshape(1, -1)\n    pred = audio_model.predict(mfcc)[0]\n    return \"happy\" if pred == 0 else \"sad\"\n\n# Function to predict facial emotion\ndef predict_face_emotion(image_data):\n    img = cv2.resize(image_data, (48, 48))\n    img = np.array(img).reshape(1, 48, 48, 1) / 255.0\n    result = face_model.predict(img)\n    label_idx = np.argmax(result[0])\n    return label_dict[label_idx]\n\n# Function to recommend songs based on the detected mood\ndef recommend_by_mood(detected_mood):\n    keywords = mood_keywords.get(detected_mood.lower(), [])\n    if not keywords:\n        print(\"No matching keywords for mood:\", detected_mood)\n        return\n\n    # Here, replace with your song recommendation logic (filter by mood)\n    print(f\"\\nüéß Mood: {detected_mood.upper()} ‚Äî Top Song Recommendations:\")\n\n# Function to combine audio and facial emotions\ndef combine_emotions(audio_path, face_image):\n    audio_emotion = predict_audio_emotion(audio_path)\n    face_emotion = predict_face_emotion(face_image)\n    \n    if audio_emotion.lower() == face_emotion.lower():\n        final = face_emotion\n    elif face_emotion in ['Happy', 'Sad']:\n        final = face_emotion\n    else:\n        final = audio_emotion.capitalize()\n\n    return final\n\n# Start webcam capture (facial emotion)\ndef capture_face_and_predict():\n    cap = cv2.VideoCapture(0)  # Webcam\n    while True:\n        ret, frame = cap.read()\n        if not ret:\n            break\n        face_emotion = predict_face_emotion(frame)\n        print(\"Facial Emotion:\", face_emotion)\n        \n        # After predicting, pass this emotion to the recommender system\n        recommend_by_mood(face_emotion)  # Or combine face and audio emotion\n\n        # Show live webcam feed (optional)\n        cv2.imshow(\"Face Emotion Detection\", frame)\n        \n        if cv2.waitKey(1) & 0xFF == ord('q'):\n            break\n    \n    cap.release()\n    cv2.destroyAllWindows()\n\n# Start audio capture (audio emotion)\ndef capture_audio_and_predict():\n    # Use Sounddevice to capture live audio from microphone\n    def audio_callback(indata, frames, time, status):\n        if status:\n            print(status)\n        audio_emotion = predict_audio_emotion(indata)\n        print(\"Audio Emotion:\", audio_emotion)\n        \n        # Use audio emotion to recommend songs\n        recommend_by_mood(audio_emotion)\n\n    # Set up audio stream\n    with sd.InputStream(callback=audio_callback, channels=1, samplerate=16000):\n        while True:\n            time.sleep(0.1)\n\n# Run both captures in parallel (audio + video)\ndef run_live_system():\n    # Run audio capture in a separate thread\n    audio_thread = threading.Thread(target=capture_audio_and_predict)\n    audio_thread.start()\n\n    # Run facial emotion detection in main thread\n    capture_face_and_predict()\n\n# Start the live system\nrun_live_system()\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-05-01T04:21:23.964636Z","iopub.execute_input":"2025-05-01T04:21:23.964877Z","iopub.status.idle":"2025-05-01T04:21:24.006873Z","shell.execute_reply.started":"2025-05-01T04:21:23.964860Z","shell.execute_reply":"2025-05-01T04:21:24.005953Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m/tmp/ipykernel_31/667941650.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0msounddevice\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0msd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mlibrosa\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mcv2\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mthreading\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'sounddevice'"],"ename":"ModuleNotFoundError","evalue":"No module named 'sounddevice'","output_type":"error"}],"execution_count":21},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}